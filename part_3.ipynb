{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3-jYfmwbPcw",
        "outputId": "3d15bba8-e8d1-48c1-f80c-05dca13e5228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo\n",
            "  Downloading pymongo-4.15.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading pymongo-4.15.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.8.0 pymongo-4.15.5\n"
          ]
        }
      ],
      "source": [
        "! pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bson"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqmgY9-jnK97",
        "outputId": "22e020e1-3a3d-4999-b0bf-1295dd946722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bson in /usr/local/lib/python3.12/dist-packages (0.5.10)\n",
            "Requirement already satisfied: python-dateutil>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from bson) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from bson) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "6bYx2GLeF3bJ",
        "outputId": "7da4da34-fbf5-4c51-ae47-f14023e64f0a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'SON' from 'bson' (/usr/local/lib/python3.12/dist-packages/bson/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1537904798.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpymongo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pymongo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpymongo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_csot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpymongo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_version_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion_tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpymongo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmongo_client\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAsyncMongoClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpymongo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMAX_SUPPORTED_WIRE_VERSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMIN_SUPPORTED_WIRE_VERSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpymongo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCursorType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pymongo/asynchronous/mongo_client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodec_options\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDEFAULT_CODEC_OPTIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCodecOptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeRegistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpymongo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_csot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelpers_shared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiodic_executor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpymongo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclient_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri_parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpymongo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_stream\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAsyncChangeStream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAsyncClusterChangeStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pymongo/common.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munquote_plus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbson\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUuidRepresentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodec_options\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCodecOptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatetimeConversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeRegistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'SON' from 'bson' (/usr/local/lib/python3.12/dist-packages/bson/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Temporarily commenting out the imports to allow for reinstallation of packages.\n",
        "# from pprint import pprint\n",
        "# import urllib.parse\n",
        "# from bson.objectid import ObjectId\n",
        "# import sentence_transformers\n",
        "# import json\n",
        "# from typing import List\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import pymongo\n",
        "\n",
        "!pip uninstall -y bson pymongo\n",
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSNXQ4iOeL75"
      },
      "outputs": [],
      "source": [
        "username = urllib.parse.quote_plus(\"Vishal\")\n",
        "password = urllib.parse.quote_plus(\"USER3@\")\n",
        "MONGODB_URI = f\"mongodb+srv://{username}:{password}@cluster0.zogssch.mongodb.net/project3\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fl-ig5DeoFm"
      },
      "outputs": [],
      "source": [
        "client = pymongo.MongoClient(MONGODB_URI)\n",
        "db = client[\"project3\"]\n",
        "col = db[\"iwb_companies\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEDgcBuVZv1x"
      },
      "outputs": [],
      "source": [
        "all_df = pd.DataFrame(col.find({},{'embeddings':0}))\n",
        "all_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPRHhmAQZ4EE"
      },
      "outputs": [],
      "source": [
        "# col.create_index([('embeddings.model', pymongo.ASCENDING)])\n",
        "# col.create_index([('embeddings.input', pymongo.ASCENDING)])\n",
        "# col.create_index([('embeddings.chunk_size', pymongo.ASCENDING)])\n",
        "# col.create_index([('embeddings.aggregation', pymongo.ASCENDING)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP3JJma4e9-g"
      },
      "outputs": [],
      "source": [
        "# col.create_index([\n",
        "#     ('embeddings.model', pymongo.ASCENDING),\n",
        "#     ('embeddings.input', pymongo.ASCENDING),\n",
        "#     ('embeddings.chunk_size', pymongo.ASCENDING),\n",
        "#     ('embeddings.aggregation', pymongo.ASCENDING)\n",
        "# ], name=\"embedding_config_compound_index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPbb1RI_kg7Z"
      },
      "source": [
        "## No Chunking on wiki_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blTaj6xafBGK"
      },
      "outputs": [],
      "source": [
        "\n",
        "import sentence_transformers\n",
        "import math\n",
        "from pymongo import UpdateOne\n",
        "\n",
        "for model_str in ['BAAI/bge-small-en-v1.5',\n",
        "                  'BAAI/bge-large-en-v1.5',\n",
        "                  'sentence-transformers/all-mpnet-base-v2',\n",
        "                  'nomic classification:',\n",
        "                  'nomic clustering:',\n",
        "                  'nomic search_query:',\n",
        "                  'nomic search_document:']:\n",
        "\n",
        "  print(f'STARTING {model_str}:')\n",
        "  # --- 1. Define your configuration ---\n",
        "  embedding_config = {\n",
        "      'model': model_str,\n",
        "      'chunk_size': None,  # 'None' clearly indicates \"no chunking\"\n",
        "      'aggregation': None  # No aggregation if there are no chunks\n",
        "  }\n",
        "  print(f\"Target embedding config: {embedding_config}\")\n",
        "\n",
        "  # --- 2. Build the MongoDB query ---\n",
        "\n",
        "  # Condition 1: Find documents that need this embedding\n",
        "  # We use $elemMatch to find an element in the 'embeddings' array\n",
        "  # that matches all fields of our config.\n",
        "  # We use $not to find documents that *don't* have such an element.\n",
        "  needs_embedding_filter = {\n",
        "      \"embeddings\": {\n",
        "          \"$not\": {\n",
        "              \"$elemMatch\": {\n",
        "                  \"model\": embedding_config['model'],\n",
        "                  \"chunk_size\": embedding_config['chunk_size'],\n",
        "                  \"aggregation\": embedding_config['aggregation'],\n",
        "                  \"input\": \"wiki_content_only\",\n",
        "              }\n",
        "          }\n",
        "      }\n",
        "  }\n",
        "\n",
        "\n",
        "  # --- 3. Fetch *only* the documents that need processing ---\n",
        "  cursor = col.find(needs_embedding_filter)\n",
        "  todo_df = pd.DataFrame(cursor)\n",
        "\n",
        "\n",
        "\n",
        "  if todo_df.empty:\n",
        "      print(f\"No documents found that need this embedding config {embedding_config}.\")\n",
        "  else:\n",
        "      print(f\"Found {len(todo_df)} documents to process.\")\n",
        "      if 'nomic' in model_str:\n",
        "        model = sentence_transformers.SentenceTransformer('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True)\n",
        "      else:\n",
        "        model = sentence_transformers.SentenceTransformer(model_str, trust_remote_code=True)\n",
        "\n",
        "      # --- 4. Mini-Batch Processing ---\n",
        "      batch_size = 10\n",
        "      total_rows = len(todo_df)\n",
        "      num_batches = math.ceil(total_rows / batch_size)\n",
        "\n",
        "      print(f\"Processing in {num_batches} batches of {batch_size}...\")\n",
        "\n",
        "      # Loop through the DataFrame in steps of batch_size\n",
        "      for i in range(0, total_rows, batch_size):\n",
        "          # Get the current mini-batch\n",
        "          batch_df = todo_df.iloc[i : i + batch_size]\n",
        "\n",
        "          current_batch_num = (i // batch_size) + 1\n",
        "          print(f\"\\n--- Processing Batch {current_batch_num}/{num_batches} ---\")\n",
        "\n",
        "          # 1. Get content for this batch\n",
        "          contents_to_embed = batch_df['wiki_content'].tolist()\n",
        "          if 'nomic' in model_str:\n",
        "            prefix = model_str.split()[1]\n",
        "            contents_to_embed = [prefix+' '+str(d) for d in contents_to_embed]\n",
        "\n",
        "          if not contents_to_embed:\n",
        "              print(\"  No content in this batch, skipping.\")\n",
        "              continue\n",
        "\n",
        "          # 2. Encode *only* this batch's content\n",
        "          print(f\"  Encoding {len(contents_to_embed)} items...\")\n",
        "          batch_embeddings = model.encode(contents_to_embed, normalize_embeddings=True)\n",
        "\n",
        "          # 3. Create update operations for this batch\n",
        "          operations = []\n",
        "\n",
        "          for j, (df_index, row) in enumerate(batch_df.iterrows()):\n",
        "\n",
        "              embedding_vector = batch_embeddings[j]\n",
        "\n",
        "              # Create the dictionary object to be pushed\n",
        "              embedding_document = {\n",
        "                  'model': embedding_config['model'],\n",
        "                  'chunk_size': embedding_config['chunk_size'],\n",
        "                  'aggregation': embedding_config['aggregation'],\n",
        "                  \"input\": \"wiki_content_only\",\n",
        "                  'embedding': embedding_vector.tolist(),\n",
        "              }\n",
        "\n",
        "              # Create an UpdateOne operation\n",
        "              op = UpdateOne(\n",
        "                  {'_id': row['_id']},  # Find doc by unique _id\n",
        "                  {\n",
        "                      '$push': {\n",
        "                          'embeddings': embedding_document  # Push the new config dict\n",
        "                      }\n",
        "                  },\n",
        "                  # upsert=False is fine here, since we *know* the doc exists\n",
        "                  # But upsert=True is safer and doesn't hurt\n",
        "                  upsert=True\n",
        "              )\n",
        "              operations.append(op)\n",
        "\n",
        "          # 4. Execute bulk write for THIS batch\n",
        "          if operations:\n",
        "              print(f\"  Sending {len(operations)} updates to MongoDB...\")\n",
        "              try:\n",
        "                  col.bulk_write(operations)\n",
        "              except Exception as e:\n",
        "                  print(f\"  An error occurred during bulk write for this batch: {e}\")\n",
        "\n",
        "      print(\"\\n--- All mini-batches processed successfully. ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxSpq2lsigIy"
      },
      "outputs": [],
      "source": [
        "tmpdf=pd.DataFrame(col.find(\n",
        "  {},\n",
        "  {\n",
        "    'ticker': 1,\n",
        "    'sector': 1,\n",
        "    'embeddings': { '$elemMatch': {'model': 'BAAI/bge-large-en-v1.5',\n",
        "                                   'chunk_size': None,\n",
        "                                   'aggregation': None,\n",
        "                                   'input': 'wiki_content_only'\n",
        "                                   } }\n",
        "  }\n",
        "))\n",
        "tmpdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huhlW0GVihLK"
      },
      "outputs": [],
      "source": [
        "this_embeddings = np.array(tmpdf.embeddings.map(lambda d: np.array(d[0]['embedding'])).to_list())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QM9khbr7ir_M"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def plot_silhouette_analysis(embeddings, labels, model_name):\n",
        "    \"\"\"\n",
        "    Create a silhouette plot showing individual sample scores within each cluster/sector\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    embeddings : numpy array\n",
        "        The embedding vectors (either raw or UMAP-reduced)\n",
        "    labels : array-like\n",
        "        The sector/cluster labels for each sample\n",
        "    model_name : str\n",
        "        Name of the model for the title\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate silhouette scores for each sample\n",
        "    silhouette_vals = silhouette_samples(embeddings, labels)\n",
        "\n",
        "    # Calculate average silhouette score\n",
        "    # from sklearn.preprocessing import normalize\n",
        "    # embeddings = normalize(embeddings) # Not needed if cosine\n",
        "\n",
        "    avg_score = silhouette_score(embeddings, labels, metric='cosine')\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "\n",
        "    # Get unique labels and sort them for consistent ordering\n",
        "    unique_labels = sorted(np.unique(labels))\n",
        "    y_lower = 10\n",
        "\n",
        "    for i, sector in enumerate(unique_labels):\n",
        "        # Get silhouette scores for samples in this sector\n",
        "        sector_mask = labels == sector\n",
        "        sector_silhouette_vals = silhouette_vals[sector_mask]\n",
        "        sector_silhouette_vals.sort()\n",
        "\n",
        "        size = sector_silhouette_vals.shape[0]\n",
        "        y_upper = y_lower + size\n",
        "\n",
        "        # Create color for this sector\n",
        "        color = plt.cm.nipy_spectral(float(i) / len(unique_labels))\n",
        "\n",
        "        # Fill the area for this sector's silhouette scores\n",
        "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                         0, sector_silhouette_vals,\n",
        "                         facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their sector names\n",
        "        ax.text(-0.05, y_lower + 0.5 * size, str(sector)[:20], fontsize=8)  # Truncate long names\n",
        "\n",
        "        # Add some space between sectors\n",
        "        y_lower = y_upper + 10\n",
        "\n",
        "    # Add vertical line for average silhouette score\n",
        "    ax.axvline(x=avg_score, color=\"red\", linestyle=\"--\",\n",
        "               label=f'Average: {avg_score:.3f}')\n",
        "\n",
        "    ax.set_xlabel(\"Silhouette Coefficient\")\n",
        "    ax.set_ylabel(\"Sector\")\n",
        "    ax.set_title(f\"Silhouette Analysis for {model_name}\\n\" +\n",
        "                 f\"Average Score: {avg_score:.4f}\")\n",
        "    ax.set_xlim([-0.1, 0.3])  # Adjust based on your data range\n",
        "    ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return avg_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KPghzjdi2mU"
      },
      "outputs": [],
      "source": [
        "plot_silhouette_analysis(this_embeddings, tmpdf.sector, 'bge-large')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12Xq95xmjGaJ"
      },
      "outputs": [],
      "source": [
        "no_chunk_content_silhouette = pd.Series()\n",
        "for model_str in ['BAAI/bge-small-en-v1.5',\n",
        "                  'BAAI/bge-large-en-v1.5',\n",
        "                  'sentence-transformers/all-mpnet-base-v2',\n",
        "                  'nomic classification:',\n",
        "                  'nomic clustering:',\n",
        "                  'nomic search_query:',\n",
        "                  'nomic search_document:']:\n",
        "\n",
        "\n",
        "  tmpdf=pd.DataFrame(col.find(\n",
        "    {},\n",
        "    {\n",
        "      'ticker': 1,\n",
        "      'sector': 1,\n",
        "      'embeddings': { '$elemMatch': {'model': model_str,\n",
        "                                    'chunk_size': None,\n",
        "                                    'aggregation': None,\n",
        "                                    'input': 'wiki_content_only'\n",
        "                                    } }\n",
        "    }\n",
        "  ))\n",
        "  tmpdf = tmpdf.loc[tmpdf.embeddings.notna()]\n",
        "  this_embeddings=np.array(tmpdf.embeddings.map(lambda d: np.array(d[0]['embedding'])).to_list())\n",
        "  no_chunk_content_silhouette.loc[model_str] = silhouette_score(this_embeddings, tmpdf.sector, metric='cosine')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmvLV2igjL3Y"
      },
      "outputs": [],
      "source": [
        "no_chunk_content_silhouette.sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuueTVVZka9Y"
      },
      "source": [
        "## No Chunking on SUMMARY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27uR2ruHkEyO"
      },
      "outputs": [],
      "source": [
        "import sentence_transformers\n",
        "import math\n",
        "from pymongo import UpdateOne\n",
        "\n",
        "summary_fields=['SUMMARY_business_description',\n",
        "                'SUMMARY_investment_exposure',\n",
        "                'SUMMARY_investment_industry',\n",
        "                'SUMMARY_material_points']\n",
        "\n",
        "for model_str in ['BAAI/bge-small-en-v1.5',\n",
        "                  'BAAI/bge-large-en-v1.5',\n",
        "                  'sentence-transformers/all-mpnet-base-v2',\n",
        "                  'nomic classification:',\n",
        "                  'nomic clustering:',\n",
        "                  'nomic search_query:',\n",
        "                  'nomic search_document:']:\n",
        "\n",
        "  print(f'STARTING {model_str}:')\n",
        "  # --- 1. Define your configuration ---\n",
        "  embedding_config = {\n",
        "      'model': model_str,\n",
        "      'chunk_size': None,  # 'None' clearly indicates \"no chunking\"\n",
        "      'aggregation': None  # No aggregation if there are no chunks\n",
        "  }\n",
        "  print(f\"Target embedding config: {embedding_config}\")\n",
        "\n",
        "  # --- 2. Build the MongoDB query ---\n",
        "\n",
        "  # Condition 1: Find documents that need this embedding\n",
        "  # We use $elemMatch to find an element in the 'embeddings' array\n",
        "  # that matches all fields of our config.\n",
        "  # We use $not to find documents that *don't* have such an element.\n",
        "  needs_embedding_filter = {\n",
        "      \"embeddings\": {\n",
        "          \"$not\": {\n",
        "              \"$elemMatch\": {\n",
        "                  \"model\": embedding_config['model'],\n",
        "                  \"chunk_size\": embedding_config['chunk_size'],\n",
        "                  \"aggregation\": embedding_config['aggregation'],\n",
        "                  \"input\": \"SUMMARY_only\",\n",
        "              }\n",
        "          }\n",
        "      },\n",
        "      \"wiki_content\": {\"$exists\": True},  # Make sure we have content\n",
        "      \"SUMMARY_material_points\": {\"$exists\": True} # The \"to-do\" flag\n",
        "  }\n",
        "\n",
        "\n",
        "  # --- 3. Fetch *only* the documents that need processing ---\n",
        "  cursor = col.find(needs_embedding_filter,\n",
        "                           {'_id': 1,\n",
        "                            'ticker': 1,\n",
        "                            'sector': 1,\n",
        "                            'SUMMARY_business_description':1,\n",
        "                            'SUMMARY_investment_exposure':1,\n",
        "                            'SUMMARY_investment_industry':1,\n",
        "                            'SUMMARY_material_points':1,\n",
        "                            'wiki_content':1\n",
        "                           }\n",
        "\n",
        "                           )\n",
        "  todo_df = pd.DataFrame(cursor)\n",
        "\n",
        "\n",
        "\n",
        "  if todo_df.empty:\n",
        "      print(f\"No documents found that need this embedding config {embedding_config}.\")\n",
        "  else:\n",
        "      print(f\"Found {len(todo_df)} documents to process.\")\n",
        "      if 'nomic' in model_str:\n",
        "        model = sentence_transformers.SentenceTransformer('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True)\n",
        "      else:\n",
        "        model = sentence_transformers.SentenceTransformer(model_str, trust_remote_code=True)\n",
        "\n",
        "      # --- 4. Mini-Batch Processing ---\n",
        "      batch_size = 10\n",
        "      total_rows = len(todo_df)\n",
        "      num_batches = math.ceil(total_rows / batch_size)\n",
        "\n",
        "      print(f\"Processing in {num_batches} batches of {batch_size}...\")\n",
        "\n",
        "      # Loop through the DataFrame in steps of batch_size\n",
        "      for i in range(0, total_rows, batch_size):\n",
        "          # Get the current mini-batch\n",
        "          batch_df = todo_df.iloc[i : i + batch_size]\n",
        "\n",
        "          current_batch_num = (i // batch_size) + 1\n",
        "          print(f\"\\n--- Processing Batch {current_batch_num}/{num_batches} ---\")\n",
        "\n",
        "          # 1. Get content for this batch\n",
        "          contents_to_embed = [str(d) for d in batch_df[summary_fields].to_dict(orient='records')]\n",
        "          if 'nomic' in model_str:\n",
        "            prefix = model_str.split()[1]\n",
        "            contents_to_embed = [prefix+' '+str(d) for d in contents_to_embed]\n",
        "\n",
        "          if not contents_to_embed:\n",
        "              print(\"  No content in this batch, skipping.\")\n",
        "              continue\n",
        "\n",
        "          # 2. Encode *only* this batch's content\n",
        "          print(f\"  Encoding {len(contents_to_embed)} items...\")\n",
        "          batch_embeddings = model.encode(contents_to_embed, normalize_embeddings=True)\n",
        "\n",
        "          # 3. Create update operations for this batch\n",
        "          operations = []\n",
        "\n",
        "          for j, (df_index, row) in enumerate(batch_df.iterrows()):\n",
        "\n",
        "              embedding_vector = batch_embeddings[j]\n",
        "\n",
        "              # Create the dictionary object to be pushed\n",
        "              embedding_document = {\n",
        "                  'model': embedding_config['model'],\n",
        "                  'chunk_size': embedding_config['chunk_size'],\n",
        "                  'aggregation': embedding_config['aggregation'],\n",
        "                  \"input\": \"SUMMARY_only\",\n",
        "                  'embedding': embedding_vector.tolist(),\n",
        "              }\n",
        "\n",
        "              # Create an UpdateOne operation\n",
        "              op = UpdateOne(\n",
        "                  {'_id': row['_id']},  # Find doc by unique _id\n",
        "                  {\n",
        "                      '$push': {\n",
        "                          'embeddings': embedding_document  # Push the new config dict\n",
        "                      }\n",
        "                  },\n",
        "                  # upsert=False is fine here, since we *know* the doc exists\n",
        "                  # But upsert=True is safer and doesn't hurt\n",
        "                  upsert=True\n",
        "              )\n",
        "              operations.append(op)\n",
        "\n",
        "          # 4. Execute bulk write for THIS batch\n",
        "          if operations:\n",
        "              print(f\"  Sending {len(operations)} updates to MongoDB...\")\n",
        "              try:\n",
        "                  col.bulk_write(operations)\n",
        "              except Exception as e:\n",
        "                  print(f\"  An error occurred during bulk write for this batch: {e}\")\n",
        "\n",
        "      print(\"\\n--- All mini-batches processed successfully. ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7m_BU90jQjS"
      },
      "outputs": [],
      "source": [
        "cursor = col.find({'ticker':'TSLA'},\n",
        "                           {'_id': 1,\n",
        "                            'ticker': 1,\n",
        "                            'sector': 1,\n",
        "                            'SUMMARY_business_description':1,\n",
        "                            'SUMMARY_investment_exposure':1,\n",
        "                            'SUMMARY_investment_industry':1,\n",
        "                            'SUMMARY_material_points':1,\n",
        "                            'wiki_content':1\n",
        "                           }\n",
        "\n",
        "                           )\n",
        "todo_df = pd.DataFrame(cursor)\n",
        "str(todo_df[summary_fields].to_dict('records')[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhs3LaoBjait"
      },
      "outputs": [],
      "source": [
        "no_chunk_summary_silhouette = pd.Series()\n",
        "for model_str in ['BAAI/bge-small-en-v1.5',\n",
        "                  'BAAI/bge-large-en-v1.5',\n",
        "                  'sentence-transformers/all-mpnet-base-v2',\n",
        "                  'nomic classification:',\n",
        "                  'nomic clustering:',\n",
        "                  'nomic search_query:',\n",
        "                  'nomic search_document:']:\n",
        "\n",
        "  tmpdf=pd.DataFrame(col.find(\n",
        "     {\"wiki_content\": {\"$exists\": True},  # Make sure we have content\n",
        "      \"SUMMARY_material_points\": {\"$exists\": True} # The \"to-do\" flag\n",
        "     },\n",
        "    {\n",
        "      'ticker': 1,\n",
        "      'sector': 1,\n",
        "      'embeddings': { '$elemMatch': {'model': model_str,\n",
        "                                    'chunk_size': None,\n",
        "                                    'aggregation': None,\n",
        "                                    'input': 'SUMMARY_only'\n",
        "                                    } }\n",
        "    }\n",
        "  ))\n",
        "  tmpdf = tmpdf.loc[tmpdf.embeddings.notna()]\n",
        "  this_embeddings = np.array(tmpdf.embeddings.map(lambda d: np.array(d[0]['embedding'])).to_list())\n",
        "  no_chunk_summary_silhouette.loc[model_str] = silhouette_score(this_embeddings, tmpdf.sector, metric='cosine')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcmeM5oDmu8X"
      },
      "outputs": [],
      "source": [
        "no_chunk_summary_silhouette.sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGXr4A3GmyoV"
      },
      "outputs": [],
      "source": [
        "tmpdf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4Gp6iK_m0r7"
      },
      "outputs": [],
      "source": [
        "plot_silhouette_analysis(this_embeddings, tmpdf.sector, 'bge-large')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUZV16etr8yH"
      },
      "source": [
        "# Why nomic fails and succeed?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtCHyrYPruLb"
      },
      "outputs": [],
      "source": [
        "check_df = pd.DataFrame(col.find({'ticker':{'$in':['WMT','MSFT','TSLA','META']}}))\n",
        "check_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz8_82TGsMSE"
      },
      "outputs": [],
      "source": [
        "row = check_df.iloc[0]\n",
        "text = row.wiki_content\n",
        "sector = row.sector\n",
        "row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCkezRZ7sOJL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 2. MODEL SETUP ---\n",
        "\n",
        "print(\"Loading Nomic model (nomic-ai/nomic-embed-text-v1.5)...\")\n",
        "# Load the single model we are testing\n",
        "model = sentence_transformers.SentenceTransformer(\n",
        "    'nomic-ai/nomic-embed-text-v1.5',\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# We only care about the best-performing prefix\n",
        "prefix = 'classification: '\n",
        "\n",
        "# --- 3. TEXT PREPARATION ---\n",
        "\n",
        "# a) Re-create the high-signal \"SUMMARY_only\" text\n",
        "summary_fields = [\n",
        "    'SUMMARY_business_description',\n",
        "    'SUMMARY_investment_exposure',\n",
        "    'SUMMARY_investment_industry',\n",
        "    'SUMMARY_material_points'\n",
        "]\n",
        "summary_parts = [str(row.get(field, '')) for field in summary_fields]\n",
        "summary_text = \" \".join(summary_parts)\n",
        "\n",
        "word_count_summary = len(summary_text.split())\n",
        "print(f\"  - Created 'summary_text' ({word_count_summary} words)\")\n",
        "\n",
        "# b) Create the \"synthetic_long_signal\" text\n",
        "# We repeat the pure summary text 20 times to create a document\n",
        "# that is ~8000-10000 words long, well within Nomic's stated 8192-token limit.\n",
        "# This text is 100% signal, with no noise.\n",
        "synthetic_long_signal = \" \".join([summary_text] * 20)\n",
        "\n",
        "word_count_synthetic = len(synthetic_long_signal.split())\n",
        "print(f\"  - Created 'synthetic_long_signal' ({word_count_synthetic} words)\")\n",
        "\n",
        "\n",
        "# --- 4. THE EXPERIMENT: ENCODE VECTORS ---\n",
        "\n",
        "print(\"\\nEncoding vectors...\")\n",
        "\n",
        "# Encode the short, high-signal text\n",
        "# We use normalize_embeddings=True, which is standard practice.\n",
        "vector_short_signal = model.encode(\n",
        "    prefix + summary_text,\n",
        "    normalize_embeddings=True\n",
        ")\n",
        "\n",
        "print(f\"  - Generated 'vector_short_signal' (Shape: {vector_short_signal.shape})\")\n",
        "\n",
        "# Encode the synthetic LONG, high-signal text\n",
        "vector_long_signal = model.encode(\n",
        "    prefix + synthetic_long_signal,\n",
        "    normalize_embeddings=True\n",
        ")\n",
        "\n",
        "print(f\"  - Generated 'vector_long_signal' (Shape: {vector_long_signal.shape})\")\n",
        "\n",
        "# --- 5. ANALYSIS & CONCLUSION ---\n",
        "\n",
        "# Since the vectors are L2-normalized (normalize_embeddings=True),\n",
        "# the cosine similarity is simply their dot product.\n",
        "similarity = np.dot(vector_short_signal, vector_long_signal)\n",
        "\n",
        "print(\"\\n--- [ EXPERIMENT RESULTS ] ---\")\n",
        "print(f\"Cosine Similarity between Short and Long Signal: {similarity:.6f}\")\n",
        "\n",
        "print(\"\\n--- [ CONCLUSION ] ---\")\n",
        "\n",
        "if similarity > 0.9:\n",
        "    print(\"RESULT: The vectors are nearly identical.\")\n",
        "    print(\"ANALYSIS: This proves the problem is 'Lost in the Middle' (a NOISE problem).\")\n",
        "    print(\"Nomic *can* handle 8000+ tokens of pure signal. Its failure in the main\")\n",
        "    print(\"experiment was due to the 'wiki_content' being 90% noise (history, trivia, etc.)\")\n",
        "    print(\"which *distracted* the model. Our Part 2 summarization pipeline is valuable\")\n",
        "    print(\"because it DISTILLS the signal from the noise.\")\n",
        "\n",
        "elif similarity < 0.5:\n",
        "    print(\"RESULT: The vectors are completely different.\")\n",
        "    print(\"ANALYSIS: This proves the problem is 'Mechanical Failure' (a LENGTH problem).\")\n",
        "    print(\"Nomic *cannot* handle long inputs, even if they are 100% pure signal.\")\n",
        "    print(\"The model's internal math likely 'breaks' or 'collapses' when processing\")\n",
        "    print(\"an input much longer than its 2048-token training limit.\")\n",
        "    print(\"Our Part 2 summarization pipeline is valuable not just because it distills\")\n",
        "    print(\"signal, but because it COMPRESSES it to a length the model can handle.\")\n",
        "\n",
        "else:\n",
        "    print(\"RESULT: The vectors are somewhat similar but have clearly diverged.\")\n",
        "    print(\"ANALYSIS: This suggests a combination of *both* problems.\")\n",
        "    print(\"The model is partially 'Lost in the Middle' and also suffering from\")\n",
        "    print(\"'Length Collapse' (mechanical failure) at this length. This confirms\")\n",
        "    print(\"that feeding the model a concise, short summary is the only reliable strategy.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uABokl7sS5v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTU1A0JtqTMh"
      },
      "source": [
        "## First 500 words on wiki_content_only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9Mb0PFRnEAK"
      },
      "outputs": [],
      "source": [
        "\n",
        "import sentence_transformers\n",
        "import math\n",
        "from pymongo import UpdateOne\n",
        "\n",
        "for model_str in ['BAAI/bge-small-en-v1.5', 'BAAI/bge-large-en-v1.5', 'sentence-transformers/all-mpnet-base-v2', 'nomic classification:', 'nomic clustering:', 'nomic search_query:', 'nomic search_document:']:\n",
        "\n",
        "  print(f'STARTING {model_str}:')\n",
        "  # --- 1. Define your configuration ---\n",
        "  embedding_config = {\n",
        "      'model': model_str,\n",
        "      'chunk_size': 500,\n",
        "      'aggregation': 'first'\n",
        "  }\n",
        "  print(f\"Target embedding config: {embedding_config}\")\n",
        "\n",
        "  # --- 2. Build the MongoDB query ---\n",
        "\n",
        "  # Condition 1: Find documents that need this embedding\n",
        "  # We use $elemMatch to find an element in the 'embeddings' array\n",
        "  # that matches all fields of our config.\n",
        "  # We use $not to find documents that *don't* have such an element.\n",
        "  needs_embedding_filter = {\n",
        "      \"embeddings\": {\n",
        "          \"$not\": {\n",
        "              \"$elemMatch\": {\n",
        "                  \"model\": embedding_config['model'],\n",
        "                  \"chunk_size\": embedding_config['chunk_size'],\n",
        "                  \"aggregation\": embedding_config['aggregation'],\n",
        "                  \"input\": \"wiki_content_only\",\n",
        "              }\n",
        "          }\n",
        "      }\n",
        "  }\n",
        "\n",
        "\n",
        "  # --- 3. Fetch *only* the documents that need processing ---\n",
        "  cursor = col.find(needs_embedding_filter)\n",
        "  todo_df = pd.DataFrame(cursor)\n",
        "  if not 'wiki_content' in todo_df.columns:\n",
        "    continue\n",
        "  if not len(todo_df):\n",
        "    continue\n",
        "  todo_df = todo_df.loc[todo_df.wiki_content.notna()]\n",
        "\n",
        "\n",
        "  if todo_df.empty:\n",
        "      print(\"No documents found that need this embedding config. All done.\")\n",
        "  else:\n",
        "      print(f\"Found {len(todo_df)} documents to process.\")\n",
        "      if 'nomic' in model_str:\n",
        "        model = sentence_transformers.SentenceTransformer('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True)\n",
        "      else:\n",
        "        model = sentence_transformers.SentenceTransformer(model_str, trust_remote_code=True)\n",
        "\n",
        "      # --- 4. Mini-Batch Processing ---\n",
        "      batch_size = 10\n",
        "      total_rows = len(todo_df)\n",
        "      num_batches = math.ceil(total_rows / batch_size)\n",
        "\n",
        "      print(f\"Processing in {num_batches} batches of {batch_size}...\")\n",
        "\n",
        "      # Loop through the DataFrame in steps of batch_size\n",
        "      for i in range(0, total_rows, batch_size):\n",
        "          # Get the current mini-batch\n",
        "          batch_df = todo_df.iloc[i : i + batch_size]\n",
        "\n",
        "          current_batch_num = (i // batch_size) + 1\n",
        "          print(f\"\\n--- Processing Batch {current_batch_num}/{num_batches} ---\")\n",
        "\n",
        "          # 1. Get content for this batch\n",
        "          contents_to_embed = batch_df['wiki_content'].tolist()\n",
        "          if embedding_config['chunk_size'] and embedding_config['aggregation'] == 'first':\n",
        "            contents_to_embed = [' '.join(d.split()[:embedding_config['chunk_size']]) for d in contents_to_embed]\n",
        "          if 'nomic' in model_str:\n",
        "            prefix = model_str.split()[1]\n",
        "            contents_to_embed = [prefix+' '+str(d) for d in contents_to_embed]\n",
        "\n",
        "\n",
        "          if not contents_to_embed:\n",
        "              print(\"  No content in this batch, skipping.\")\n",
        "              continue\n",
        "\n",
        "          # 2. Encode *only* this batch's content\n",
        "          print(f\"  Encoding {len(contents_to_embed)} items...\")\n",
        "          batch_embeddings = model.encode(contents_to_embed, normalize_embeddings=True)\n",
        "\n",
        "          # 3. Create update operations for this batch\n",
        "          operations = []\n",
        "\n",
        "          for j, (df_index, row) in enumerate(batch_df.iterrows()):\n",
        "\n",
        "              embedding_vector = batch_embeddings[j]\n",
        "\n",
        "              # Create the dictionary object to be pushed\n",
        "              embedding_document = {\n",
        "                  'model': embedding_config['model'],\n",
        "                  'chunk_size': embedding_config['chunk_size'],\n",
        "                  'aggregation': embedding_config['aggregation'],\n",
        "                  \"input\": \"wiki_content_only\",\n",
        "                  'embedding': embedding_vector.tolist(),\n",
        "              }\n",
        "\n",
        "              # Create an UpdateOne operation\n",
        "              op = UpdateOne(\n",
        "                  {'ticker': row['ticker']},  # Find doc by ticker\n",
        "                  {\n",
        "                      '$push': {\n",
        "                          'embeddings': embedding_document  # Push the new config dict\n",
        "                      }\n",
        "                  },\n",
        "                  # upsert=False is fine here, since we *know* the doc exists\n",
        "                  # But upsert=True is safer and doesn't hurt\n",
        "                  upsert=True\n",
        "              )\n",
        "              operations.append(op)\n",
        "\n",
        "          # 4. Execute bulk write for THIS batch\n",
        "          if operations:\n",
        "              print(f\"  Sending {len(operations)} updates to MongoDB...\")\n",
        "              try:\n",
        "                  col.bulk_write(operations)\n",
        "              except Exception as e:\n",
        "                  print(f\"  An error occurred during bulk write for this batch: {e}\")\n",
        "\n",
        "      print(\"\\n--- All mini-batches processed successfully. ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7H0ultlqqPZI"
      },
      "outputs": [],
      "source": [
        "aggregation_summary_silhouette = pd.Series()\n",
        "for model_str in ['BAAI/bge-small-en-v1.5', 'BAAI/bge-large-en-v1.5', 'sentence-transformers/all-mpnet-base-v2', 'nomic classification:', 'nomic clustering:', 'nomic search_query:', 'nomic search_document:']:\n",
        "\n",
        "  tmpdf=pd.DataFrame(col.find(\n",
        "     {\"wiki_content\": {\"$exists\": True},  # Make sure we have content\n",
        "      \"SUMMARY_material_points\": {\"$exists\": True} # The \"to-do\" flag\n",
        "     },\n",
        "    {\n",
        "      'ticker': 1,\n",
        "      'sector': 1,\n",
        "      'embeddings': { '$elemMatch': {\n",
        "          'model': model_str,\n",
        "          'chunk_size': 500,  # 'None' clearly indicates \"no chunking\"\n",
        "          'aggregation': 'first',  # No aggregation if there are no chunks\n",
        "          \"input\": \"wiki_content_only\",\n",
        "                                    } }\n",
        "    }\n",
        "  ))\n",
        "  tmpdf = tmpdf.loc[tmpdf.embeddings.notna()]\n",
        "  this_embeddings=np.array(tmpdf.embeddings.map(lambda d: np.array(d[0]['embedding'])).to_list())\n",
        "  aggregation_summary_silhouette.loc[model_str] = silhouette_score(this_embeddings, tmpdf.sector, metric='cosine')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibPqFKNkrphr"
      },
      "outputs": [],
      "source": [
        "aggregation_summary_silhouette.sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyxC8dYly219"
      },
      "source": [
        "# 3.7 Extra Credit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fiEUhTZzsx0"
      },
      "outputs": [],
      "source": [
        "def chunk_text_by_words(text, chunk_size=500, overlap=50, prefix=''):\n",
        "    \"\"\"\n",
        "    Chunk text by word count with overlap\n",
        "    \"\"\"\n",
        "    try:\n",
        "      words = text.split()\n",
        "      chunks = []\n",
        "      for i in range(0, len(words), chunk_size - overlap):\n",
        "          chunk = ' '.join(words[i:i + chunk_size])\n",
        "          chunks.append(prefix+chunk)\n",
        "          if i + chunk_size >= len(words):\n",
        "              break\n",
        "      return chunks\n",
        "    except:\n",
        "      return [prefix+text]\n",
        "\n",
        "def exponential_weight_aggregation(chunk_embeddings, decay=0.5):\n",
        "    \"\"\"\n",
        "    Weight earlier chunks more heavily (they usually contain overview/summary)\n",
        "    \"\"\"\n",
        "    weights = np.array([decay**i for i in range(len(chunk_embeddings))])\n",
        "    weights = weights / weights.sum()\n",
        "    return np.average(chunk_embeddings, weights=weights, axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU04tNSVy2jI"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  for iterative in [250,500,1000]:\n",
        "    for model_str in ['nomic clustering:', 'nomic search_query:', 'nomic search_document:','BAAI/bge-small-en-v1.5', 'BAAI/bge-large-en-v1.5', 'sentence-transformers/all-mpnet-base-v2', 'nomic classification:']:\n",
        "\n",
        "      print(f'STARTING {model_str}:')\n",
        "      if 'nomic' in model_str:\n",
        "        model = sentence_transformers.SentenceTransformer('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True)\n",
        "      else:\n",
        "        model = sentence_transformers.SentenceTransformer(model_str, trust_remote_code=True)\n",
        "\n",
        "      # --- 1. Define your configuration ---\n",
        "      embedding_config = {\n",
        "          'model': model_str,\n",
        "          'chunk_size': iterative,\n",
        "          'aggregation': 'exponential-0.5',\n",
        "          \"input\": \"wiki_content_only\",\n",
        "      }\n",
        "      print(f\"Target embedding config: {embedding_config}\")\n",
        "\n",
        "      # --- 2. Build the MongoDB query ---\n",
        "\n",
        "      # Condition 1: Find documents that need this embedding\n",
        "      # We use $elemMatch to find an element in the 'embeddings' array\n",
        "      # that matches all fields of our config.\n",
        "      # We use $not to find documents that *don't* have such an element.\n",
        "\n",
        "\n",
        "      # SLOW as index is not used\n",
        "      # needs_embedding_filter = {\n",
        "      #     \"embeddings\": {\n",
        "      #         \"$not\": {\n",
        "      #             \"$elemMatch\": {\n",
        "      #                 \"model\": embedding_config['model'],\n",
        "      #                 \"chunk_size\": embedding_config['chunk_size'],\n",
        "      #                 \"aggregation\": embedding_config['aggregation'],\n",
        "      #                 \"input\": embedding_config['input'],\n",
        "      #             }\n",
        "      #         }\n",
        "      #     }\n",
        "      # }\n",
        "      # cursor = col.find(needs_embedding_filter, {'embeddings':-1})\n",
        "      # todo_df = pd.DataFrame(cursor)\n",
        "\n",
        "      has_embedding_cursor = col.find(\n",
        "          {\n",
        "              \"embeddings\": {\n",
        "                  \"$elemMatch\": {\n",
        "                      \"model\": embedding_config['model'],\n",
        "                      \"chunk_size\": embedding_config['chunk_size'],\n",
        "                      \"aggregation\": embedding_config['aggregation'],\n",
        "                      \"input\": embedding_config['input'],\n",
        "                  }\n",
        "              }\n",
        "          },\n",
        "          {\"_id\": 1}\n",
        "      )\n",
        "      has_embedding_ids = {doc['_id'] for doc in has_embedding_cursor}\n",
        "\n",
        "      for row in col.find(\n",
        "              {\"_id\": {\"$nin\": list(has_embedding_ids)}},\n",
        "              {'embeddings': 0}\n",
        "          ):\n",
        "\n",
        "          # print(row['ticker'])\n",
        "\n",
        "\n",
        "          # 1. Get content for this batch\n",
        "          try:\n",
        "            content_to_embed = row['wiki_content']\n",
        "          except:\n",
        "            continue\n",
        "\n",
        "          if len(content_to_embed)<10:\n",
        "              print(\"  No content in this batch, skipping.\")\n",
        "              continue\n",
        "\n",
        "          chunks = chunk_text_by_words(content_to_embed, chunk_size=embedding_config['chunk_size'], overlap=50)\n",
        "\n",
        "          if 'nomic' in model_str:\n",
        "            prefix = model_str.split()[1]\n",
        "            chunks = [prefix+' '+str(d) for d in chunks]\n",
        "\n",
        "          # 2. Encode chunks\n",
        "          print(f\"  Encoding {len(chunks)} chunks...\")\n",
        "          chunk_embeddings = model.encode(chunks, normalize_embeddings=True)\n",
        "\n",
        "          if embedding_config['aggregation'].startswith('exponential'):\n",
        "            decay = float(embedding_config['aggregation'].split('-')[1])\n",
        "            aggregated_embedding = exponential_weight_aggregation(chunk_embeddings, decay=decay)\n",
        "          else:\n",
        "            aggregated_embedding = np.mean(chunk_embeddings, axis=0)\n",
        "\n",
        "          embedding_document = {\n",
        "              'model': embedding_config['model'],\n",
        "              'chunk_size': embedding_config['chunk_size'],\n",
        "              'aggregation': embedding_config['aggregation'],\n",
        "              \"input\": embedding_config['input'],\n",
        "              'embedding': aggregated_embedding.tolist(),\n",
        "          }\n",
        "\n",
        "          col.update_one(\n",
        "              {'_id':row['_id']},\n",
        "              {\n",
        "                  '$push': {\n",
        "                      'embeddings': embedding_document\n",
        "                  }\n",
        "              }\n",
        "          )\n",
        "\n",
        "          print(\"\\n--- All processed successfully. ---\")\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGlwuvHg0X9d"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "alltic = all_df.ticker.to_list()\n",
        "yf_df = yf.download(alltic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXxhattPRxf8"
      },
      "outputs": [],
      "source": [
        "corrmat = yf_df.Close.pct_change().clip(-0.1,0.1).corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kroRW4iuR0gC"
      },
      "outputs": [],
      "source": [
        "corrmat['MSFT'].sort_values().dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztKgQEG8R2Mt"
      },
      "outputs": [],
      "source": [
        "model_at25 = pd.Series()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3kSLoUgR53M"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def precision_recall_at_k(embedding_matrix, corrmat, k_values=[5, 10, 20]):\n",
        "    embedding_sim = cosine_similarity(embedding_matrix)\n",
        "\n",
        "    results = []\n",
        "    for i, ticker in enumerate(corrmat.columns):\n",
        "        ticker_results = {'ticker': ticker}\n",
        "\n",
        "        for k in k_values:\n",
        "            # Top K from correlations\n",
        "            corr_top_k = set(corrmat[ticker].sort_values(ascending=False).iloc[1:k+1].index)\n",
        "\n",
        "            # Top K from embeddings\n",
        "            emb_indices = np.argsort(embedding_sim[i])[::-1][1:k+1]\n",
        "            emb_top_k = set([corrmat.columns[idx] for idx in emb_indices])\n",
        "\n",
        "            # Calculate metrics\n",
        "            intersection = len(corr_top_k & emb_top_k)\n",
        "            precision = intersection / k\n",
        "            recall = intersection / k  # In this case, same as precision since sets are same size\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "            ticker_results[f'precision@{k}'] = precision\n",
        "            ticker_results[f'recall@{k}'] = recall\n",
        "            ticker_results[f'f1@{k}'] = f1\n",
        "\n",
        "        results.append(ticker_results)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "for model_str in ['BAAI/bge-small-en-v1.5', 'BAAI/bge-large-en-v1.5', 'sentence-transformers/all-mpnet-base-v2', 'nomic classification:', 'nomic clustering:', 'nomic search_query:', 'nomic search_document:']:\n",
        "\n",
        "  tmpdf=pd.DataFrame(col.find(\n",
        "     {\"wiki_content\": {\"$exists\": True},  # Make sure we have content\n",
        "      \"SUMMARY_material_points\": {\"$exists\": True} # The \"to-do\" flag\n",
        "     },\n",
        "    {\n",
        "      'ticker': 1,\n",
        "      'sector': 1,\n",
        "      'embeddings': { '$elemMatch': {\n",
        "          'model': model_str,\n",
        "          'chunk_size': 500,  # 'None' clearly indicates \"no chunking\"\n",
        "          'aggregation': 'first',  # No aggregation if there are no chunks\n",
        "          \"input\": \"wiki_content_only\",\n",
        "                                    } }\n",
        "    }\n",
        "  ))\n",
        "  tmpdf = tmpdf.loc[tmpdf.embeddings.notna()]\n",
        "  this_embeddings=np.array(tmpdf.embeddings.map(lambda d: np.array(d[0]['embedding'])).to_list())\n",
        "  results_df = precision_recall_at_k(this_embeddings, corrmat.loc[tmpdf.ticker,tmpdf.ticker], k_values=[5,10,20,25,50])\n",
        "  model_at25.loc['content_first_500_' + model_str] = results_df['precision@25'].mean()\n",
        "  print(model_at25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La-baQQcR8MU"
      },
      "outputs": [],
      "source": [
        "model_at25.sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bk1Uaw5Sf0O"
      },
      "outputs": [],
      "source": [
        "model_at25.sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPp68ImQ0lvw"
      },
      "outputs": [],
      "source": [
        "pprint(col.find_one({\"ticker\":\"NVDA\"}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eaW488x0rMx"
      },
      "outputs": [],
      "source": [
        "col.distinct(\"embeddings.input\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81X2PSmu1tTi"
      },
      "outputs": [],
      "source": [
        "col.distinct(\"embeddings.aggregation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtqn2W4u2iov"
      },
      "outputs": [],
      "source": [
        "col.distinct(\"embeddings.chunk_size\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4yQpqwf2q3Y"
      },
      "outputs": [],
      "source": [
        "for inputs in col.distinct(\"embeddings.input\"):\n",
        "  for aggregations in col.distinct(\"embeddings.aggregation\"):\n",
        "    for chunk_sizes in col.distinct(\"embeddings.chunk_size\"):\n",
        "      try:\n",
        "        summary_silhouette = pd.Series()\n",
        "        for model_str in ['BAAI/bge-small-en-v1.5', 'BAAI/bge-large-en-v1.5', 'sentence-transformers/all-mpnet-base-v2', 'nomic classification:', 'nomic clustering:', 'nomic search_query:', 'nomic search_document:']:\n",
        "          print(inputs,aggregations,chunk_sizes)\n",
        "          tmpdf=pd.DataFrame(col.find(\n",
        "            {\"wiki_content\": {\"$exists\": True},  # Make sure we have content\n",
        "              \"SUMMARY_material_points\": {\"$exists\": True} # The \"to-do\" flag\n",
        "            },\n",
        "            {\n",
        "              'ticker': 1,\n",
        "              'sector': 1,\n",
        "              'embeddings': { '$elemMatch': {\n",
        "                  'model': model_str,\n",
        "                  'chunk_size': chunk_sizes,  # 'None' clearly indicates \"no chunking\"\n",
        "                  'aggregation': aggregations,  # No aggregation if there are no chunks\n",
        "                  \"input\": inputs,\n",
        "                                            } }\n",
        "            }\n",
        "          ))\n",
        "          tmpdf = tmpdf.loc[tmpdf.embeddings.notna()]\n",
        "          this_embeddings=np.array(tmpdf.embeddings.map(lambda d: np.array(d[0]['embedding'])).to_list())\n",
        "          summary_silhouette.loc[model_str] = silhouette_score(this_embeddings, tmpdf.sector, metric='cosine')\n",
        "          print(summary_silhouette.sort_values())\n",
        "          print(\"=\"*60)\n",
        "      except Exception as e:\n",
        "        print(\"Due to Storage issue this values were not able to calculate. Therefore skipping.\")\n",
        "        print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i573mZiT-FrB"
      },
      "source": [
        "#Final Hyper Parameter: nomic classification:                      0.033227\n",
        "#input:- wiki_content_only\n",
        "#aggregation:- first\n",
        "#Chunk Size:- 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6u-nuc08EXL"
      },
      "outputs": [],
      "source": [
        "# Update all documents to copy the specific embedding vector to a top-level field\n",
        "result = col.update_many(\n",
        "    {\n",
        "        'embeddings': {\n",
        "            '$elemMatch': {\n",
        "                'model': 'nomic classification:',\n",
        "                'chunk_size': 500,\n",
        "                'aggregation': \"first\",\n",
        "                'input': 'wiki_content_only'\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    [\n",
        "        {\n",
        "            '$set': {\n",
        "                'production_embedding': {\n",
        "                    '$first': {\n",
        "                        '$filter': {\n",
        "                            'input': '$embeddings',\n",
        "                            'as': 'emb',\n",
        "                            'cond': {\n",
        "                                '$and': [\n",
        "                                    {'$eq': ['$$emb.model', 'nomic classification:']},\n",
        "                                    {'$eq': ['$$emb.chunk_size', 500]},\n",
        "                                    {'$eq': ['$$emb.aggregation', \"first\"]},\n",
        "                                    {'$eq': ['$$emb.input', 'wiki_content_only']}\n",
        "                                ]\n",
        "                            }\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            '$set': {\n",
        "                'production_embedding_vector': '$production_embedding.vector'\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f\"Updated {result.modified_count} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col.find_one({'embeddings': {\n",
        "            '$elemMatch': {\n",
        "                'model': 'nomic classification:',\n",
        "                'chunk_size': 500,\n",
        "                'aggregation': \"first\",\n",
        "                'input': 'wiki_content_only'\n",
        "            }\n",
        "        }})"
      ],
      "metadata": {
        "id": "FriXs-7hQ0mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pro_search_text = col.update_many(\n",
        "  {},\n",
        "  [\n",
        "    {\n",
        "      \"$set\": {\n",
        "        \"prod_text_for_search\": {\n",
        "          \"$concat\": [\n",
        "            # business description\n",
        "            { \"$ifNull\": [\"$SUMMARY_business_description\", \"\"] },\n",
        "\n",
        "            # separator\n",
        "            \"\\n\",\n",
        "\n",
        "            # join material points array with spaces\n",
        "            {\n",
        "              \"$cond\": [\n",
        "                { \"$isArray\": \"$SUMMARY_material_points\" },\n",
        "                {\n",
        "                  \"$reduce\": {\n",
        "                    \"input\": \"$SUMMARY_material_points\",\n",
        "                    \"initialValue\": \"\",\n",
        "                    \"in\": {\n",
        "                      \"$concat\": [\n",
        "                        \"$$value\",\n",
        "                        { \"$cond\": [{ \"$eq\": [\"$$value\", \"\"] }, \"\", \"\\n\"] },\n",
        "                        { \"$concat\": [\"**\", \"$$this\"] }\n",
        "                      ]\n",
        "                    }\n",
        "                  }\n",
        "                },\n",
        "                \"\"\n",
        "              ]\n",
        "            },\n",
        "\n",
        "            \"\\n\",\n",
        "\n",
        "            # join investment industry array with spaces\n",
        "            {\n",
        "              \"$cond\": [\n",
        "                { \"$isArray\": \"$SUMMARY_investment_industry\" },\n",
        "                {\n",
        "                  \"$reduce\": {\n",
        "                    \"input\": \"$SUMMARY_investment_industry\",\n",
        "                    \"initialValue\": \"\",\n",
        "                    \"in\": {\n",
        "                      \"$concat\": [\n",
        "                        \"$$value\",\n",
        "                        { \"$cond\": [{ \"$eq\": [\"$$value\", \"\"] }, \"\", \"\\n\"] },\n",
        "                        \"$$this\"\n",
        "                      ]\n",
        "                    }\n",
        "                  }\n",
        "                },\n",
        "                \"\"\n",
        "              ]\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "cIp1j5lpV8Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = col.update_many(\n",
        "    {},\n",
        "    { \"$unset\": { \"prod_text_for_search_TEST\": \"\" } }\n",
        ")\n",
        "\n",
        "print(\"Matched:\", result.matched_count, \"Modified:\", result.modified_count)"
      ],
      "metadata": {
        "id": "eGGVa2c7hzBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qakGSvdzlEQj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}